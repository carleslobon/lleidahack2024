{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 04:33:31.354162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732419211.426438    6479 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732419211.448927    6479 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 04:33:31.632473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "%pip install sentence-transformers\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Load initial dataset\n",
    "initial_accions_df = pd.read_csv('./datasets/accions.csv')\n",
    "initial_accions_df.drop(['Usuari', 'Representat'], axis=1, inplace=True)\n",
    "initial_tramits_df = pd.read_csv('./datasets/tramits.csv')\n",
    "tramits_map = initial_tramits_df.set_index('Id')['Titol'].to_dict()\n",
    "initial_tramits_df.drop(['Titol'], axis=1, inplace=True)\n",
    "merged_df = initial_accions_df.merge(initial_tramits_df, left_on='Tramit', right_on='Id').drop(['Id'], axis=1)\n",
    "\n",
    "# Remove non-vigent\n",
    "initial_df = merged_df[merged_df['Vigent']].drop(columns=['Vigent'])\n",
    "\n",
    "# Encode action values\n",
    "df = initial_df.copy()  # If df is a subset of another dataframe, make an explicit copy first\n",
    "df['Accio_Tramit'] = df['Accio'] + '_' + df['Tramit']\n",
    "label_encoder = LabelEncoder()\n",
    "df['action_id'] = label_encoder.fit_transform(df['Accio_Tramit'])\n",
    "df.drop(['Accio', 'Tramit', 'Accio_Tramit'], axis=1, inplace=True)\n",
    "df = df.sort_values(by=['Sessio', 'Data'])\n",
    "\n",
    "# Remove repeated actions\n",
    "df = df.loc[\n",
    "    df['action_id'] != df.groupby('Sessio')['action_id'].shift()\n",
    "]\n",
    "\n",
    "# Remove sessions with less than n actions\n",
    "n = 4\n",
    "session_counts = df.groupby('Sessio').size().reset_index(name='count')\n",
    "sessions_to_keep = session_counts[session_counts['count'] >= n]\n",
    "df = df[df['Sessio'].isin(sessions_to_keep['Sessio'])]\n",
    "\n",
    "# Store sequences in a dictionary in order\n",
    "df_sorted = df.sort_values(by=['Sessio', 'Data'])\n",
    "session_sequences = {}\n",
    "for session_id, group in df_sorted.groupby('Sessio'):\n",
    "    action_sequence = group['action_id'].tolist()\n",
    "    session_sequences[session_id] = action_sequence\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    return model.encode(sentence)\n",
    "\n",
    "action_type_map = {\n",
    "    'AFIT': 'Acces a la fitxa informativa de ',\n",
    "    'AFST': 'Acces a la fitxa de solicitud de ',\n",
    "    'PFST': 'Presentacio del formulari de solicitud de ',\n",
    "}\n",
    "\n",
    "# Generate sequences embeddings\n",
    "embedding_map = {}\n",
    "embedding_dim = 384\n",
    "def get_embedding(sentence):\n",
    "    return embedding_model.encode(sentence)\n",
    "action_type_map = {\n",
    "    'AFIT': 'Acces a la fitxa informativa de ',\n",
    "    'AFST': 'Acces a la fitxa de solicitud de ',\n",
    "    'PFST': 'Presentacio del formulari de solicitud de ',\n",
    "}\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "for indx, action in enumerate(label_encoder.classes_):\n",
    "    action_type = action.split('_')[0]\n",
    "    tramit_id = action.split('_')[1]\n",
    "    action_text = action_type_map[action_type] + tramits_map[tramit_id]\n",
    "    embedding_map[indx] = get_embedding(action_text)\n",
    "sequence_data = []\n",
    "for session_id, action_sequence in session_sequences.items():\n",
    "    for i in range(len(action_sequence) - 3):\n",
    "        input1 = embedding_map[action_sequence[i]]\n",
    "        input2 = embedding_map[action_sequence[i + 1]]\n",
    "        input3 = embedding_map[action_sequence[i + 2]]\n",
    "        label = embedding_map[action_sequence[i + 3]]\n",
    "        sequence_data.append((input1, input2, input3, label))\n",
    "\n",
    "# Cleanup step\n",
    "allowed_variables = {'sequence_data', 'embedding_map', 'embedding_dim', 'num_actions', 'label_encoder', 'tramits_map'}\n",
    "current_variables = set(globals().keys())\n",
    "for variable in current_variables - allowed_variables:\n",
    "    if variable not in ['__builtins__', '__name__', '__doc__', '__package__', '__loader__', '__spec__', '__annotations__', '__file__', '__cached__']:\n",
    "        del globals()[variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "with open(\"./streamlit/data/embeddings_v2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embedding_map, f)\n",
    "with open(\"./streamlit/data/tramits_map_v2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tramits_map, f)\n",
    "label_to_encoded = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "encoded_to_label = {v: k for k, v in label_to_encoded.items()}\n",
    "\n",
    "# Save the label-to-encoded map to a file\n",
    "with open('./streamlit/data/label_to_encoded_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(label_to_encoded, f)\n",
    "\n",
    "# Save the encoded-to-label map to another file\n",
    "with open('./streamlit/data/encoded_to_label_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(encoded_to_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Check if the embeddings file exists\n",
    "if os.path.exists(\"./datasets/encoded_to_label.pkl\"):\n",
    "    with open(\"./datasets/encoded_to_label.pkl\", \"rb\") as f:\n",
    "        embeddings = pickle.load(f)\n",
    "else:\n",
    "    embeddings = {}  # If not found, return an empty dictionary\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_map = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Input, LSTM, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerard/miniconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2ms/step - accuracy: 0.4220 - loss: 6.5343e-04 - val_accuracy: 0.4265 - val_loss: 5.7338e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2ms/step - accuracy: 0.4451 - loss: 5.9420e-04 - val_accuracy: 0.5167 - val_loss: 5.7012e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2ms/step - accuracy: 0.4457 - loss: 5.8911e-04 - val_accuracy: 0.4108 - val_loss: 5.6588e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2ms/step - accuracy: 0.4426 - loss: 5.8768e-04 - val_accuracy: 0.4131 - val_loss: 5.6461e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 5.8450e-04 - val_accuracy: 0.4749 - val_loss: 5.6450e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2ms/step - accuracy: 0.4416 - loss: 5.8562e-04 - val_accuracy: 0.4211 - val_loss: 5.6119e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2ms/step - accuracy: 0.4444 - loss: 5.8429e-04 - val_accuracy: 0.4187 - val_loss: 5.6706e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2ms/step - accuracy: 0.4448 - loss: 5.8074e-04 - val_accuracy: 0.4193 - val_loss: 5.6178e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2ms/step - accuracy: 0.4464 - loss: 5.8001e-04 - val_accuracy: 0.5269 - val_loss: 5.6217e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2ms/step - accuracy: 0.4492 - loss: 5.7824e-04 - val_accuracy: 0.4798 - val_loss: 5.6214e-04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, sequence_data, batch_size=16):\n",
    "        self.sequence_data = sequence_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence_data) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.sequence_data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X = []\n",
    "        y = []\n",
    "        for seq in batch_data:\n",
    "            X.append(np.stack(seq[:3]))\n",
    "            y.append(seq[3])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage\n",
    "batch_size = 16\n",
    "train_generator = DataGenerator(sequence_data[:int(len(sequence_data) * 0.8)], batch_size=batch_size)\n",
    "test_generator = DataGenerator(sequence_data[int(len(sequence_data) * 0.8):], batch_size=batch_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=test_generator, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerard/miniconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(3, embedding_dim)),\n",
    "    SimpleRNN(32, activation='tanh', input_shape=(3, embedding_dim)),  # 3 timesteps, 1 feature\n",
    "    Dense(embedding_dim, activation='linear')  # Cambia softmax según el tipo de tarea\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 04:35:03.949332: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/gerard/miniconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, activation='tanh', input_shape=(3, embedding_dim), return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(embedding_dim, activation='linear')  # 50 clases en la salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerard/miniconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(3, embedding_dim)),\n",
    "    GRU(64, activation='tanh', input_shape=(3, embedding_dim), return_sequences=False),  # GRU en lugar de SimpleRNN\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(embedding_dim, activation='linear')  # Cambia softmax según el tipo de tarea\n",
    "])\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 03:23:21.313130: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 548346600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1ms/step - accuracy: 0.3551 - loss: 0.6126 - val_accuracy: 0.3639 - val_loss: 0.5782\n",
      "Epoch 2/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1ms/step - accuracy: 0.3811 - loss: 0.5772 - val_accuracy: 0.3815 - val_loss: 0.5743\n",
      "Epoch 3/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1ms/step - accuracy: 0.3824 - loss: 0.5730 - val_accuracy: 0.3851 - val_loss: 0.5722\n",
      "Epoch 4/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1000us/step - accuracy: 0.3827 - loss: 0.5701 - val_accuracy: 0.3923 - val_loss: 0.5695\n",
      "Epoch 5/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1ms/step - accuracy: 0.3816 - loss: 0.5683 - val_accuracy: 0.3777 - val_loss: 0.5700\n",
      "Epoch 6/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 997us/step - accuracy: 0.3820 - loss: 0.5677 - val_accuracy: 0.3726 - val_loss: 0.5683\n",
      "Epoch 7/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 968us/step - accuracy: 0.3826 - loss: 0.5675 - val_accuracy: 0.3614 - val_loss: 0.5694\n",
      "Epoch 8/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 974us/step - accuracy: 0.3786 - loss: 0.5668 - val_accuracy: 0.3740 - val_loss: 0.5689\n",
      "Epoch 9/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 976us/step - accuracy: 0.3806 - loss: 0.5663 - val_accuracy: 0.3710 - val_loss: 0.5668\n",
      "Epoch 10/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 987us/step - accuracy: 0.3795 - loss: 0.5654 - val_accuracy: 0.3812 - val_loss: 0.5663\n",
      "Epoch 11/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 988us/step - accuracy: 0.3811 - loss: 0.5660 - val_accuracy: 0.3779 - val_loss: 0.5665\n",
      "Epoch 12/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 997us/step - accuracy: 0.3812 - loss: 0.5643 - val_accuracy: 0.3714 - val_loss: 0.5658\n",
      "Epoch 13/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 978us/step - accuracy: 0.3807 - loss: 0.5640 - val_accuracy: 0.3815 - val_loss: 0.5661\n",
      "Epoch 14/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 994us/step - accuracy: 0.3791 - loss: 0.5635 - val_accuracy: 0.3820 - val_loss: 0.5666\n",
      "Epoch 15/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 981us/step - accuracy: 0.3780 - loss: 0.5636 - val_accuracy: 0.3863 - val_loss: 0.5648\n",
      "Epoch 16/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1ms/step - accuracy: 0.3790 - loss: 0.5628 - val_accuracy: 0.3789 - val_loss: 0.5661\n",
      "Epoch 17/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1ms/step - accuracy: 0.3784 - loss: 0.5638 - val_accuracy: 0.3889 - val_loss: 0.5662\n",
      "Epoch 18/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1ms/step - accuracy: 0.3794 - loss: 0.5626 - val_accuracy: 0.3763 - val_loss: 0.5660\n",
      "Epoch 19/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 995us/step - accuracy: 0.3808 - loss: 0.5631 - val_accuracy: 0.4198 - val_loss: 0.5652\n",
      "Epoch 20/20\n",
      "\u001b[1m57120/57120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1ms/step - accuracy: 0.3829 - loss: 0.5626 - val_accuracy: 0.3866 - val_loss: 0.5664\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('./streamlit/models/model_lstm_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"
     ]
    }
   ],
   "source": [
    "random_input = random.choice(sequence_data)\n",
    "# print(f'Random input: {random_input}')\n",
    "# Stack them to form a sequence of shape (3, 50)\n",
    "input_sequence = np.stack(random_input[:3])\n",
    "# Add batch dimension (shape becomes (1, 3, 50))\n",
    "input_sequence = np.expand_dims(input_sequence, axis=0)\n",
    "predicted_output = model.predict(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action ID: 462, Similarity: 1.0\n",
      "Name: Certificats del padró d'habitants. Expedició automàtica\n",
      "Acció: AFST\n",
      "Action ID: 440, Similarity: 0.9999998807907104\n",
      "Name: Sol·licitud genèrica\n",
      "Acció: AFIT\n",
      "Action ID: 445, Similarity: 0.9999998807907104\n",
      "Name: Padró d'Habitants: gestions d'alta, canvi de domicili, sol·licitud de certificats i altres gestions relacionades\n",
      "Acció: AFIT\n",
      "Action ID: 223, Similarity: 0.9863439798355103\n",
      "Name: Sol·licitar certificats d'empadronament\n",
      "Acció: AFIT\n"
     ]
    }
   ],
   "source": [
    "for output in [random_input[0], random_input[1], random_input[2], predicted_output]:\n",
    "    output = output.flatten()\n",
    "    similarities = []\n",
    "    # Iterate through each action_id and embedding in the embeddings_map\n",
    "    for action_id, embedding in embedding_map.items():\n",
    "        # Reshape the embedding to ensure it's 2D (1, 50)\n",
    "        embedding = embedding.reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity (output is also reshaped to (1, 50) for comparison)\n",
    "        similarity = cosine_similarity([output], embedding)[0][0]\n",
    "\n",
    "        # Append the similarity and corresponding action_id to the list\n",
    "        similarities.append((action_id, similarity))\n",
    "\n",
    "    # Sort the similarities in descending order (most similar first)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 3 most similar embeddings\n",
    "    top_3_similar = similarities[:3]\n",
    "\n",
    "    # Print the top 3 closest embeddings\n",
    "    for action_id, similarity in top_3_similar[:1]:\n",
    "        print(f\"Action ID: {action_id}, Similarity: {similarity}\\nName: {tramits_map[label_encoder.inverse_transform([action_id])[0].split('_')[1]]}\\nAcció: {label_encoder.inverse_transform([action_id])[0].split('_')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.5936548  -0.47816718 -1.1702312  ... -0.41135162  0.0239204\n",
      "   -0.8581901 ]\n",
      "  [ 0.6404202  -1.9613961   0.9597868  ...  1.845338    0.41126075\n",
      "   -1.3003867 ]\n",
      "  [ 0.15095814 -1.2164788   0.34207943 ...  2.0424657  -1.0943674\n",
      "   -0.34722838]]\n",
      "\n",
      " [[-1.1344854  -2.8561525   0.11432025 ...  1.3501385   0.19619687\n",
      "    0.01653572]\n",
      "  [-0.85601515 -0.86147314 -0.37468165 ...  0.7338877   0.5312311\n",
      "   -0.9679297 ]\n",
      "  [-0.9830078  -1.4014475   2.928438   ... -2.3828523  -0.23451519\n",
      "   -1.3053267 ]]\n",
      "\n",
      " [[-0.9917925   0.93422335 -0.15956964 ... -1.0005369  -1.1052504\n",
      "    1.0878924 ]\n",
      "  [-0.538926    0.03241725 -1.7057962  ... -0.5711149   0.1057082\n",
      "    0.6349573 ]\n",
      "  [ 0.46988806  0.42448315  1.0847745  ... -0.48395592 -0.13236533\n",
      "   -0.16239367]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.09672425  0.10088343 -0.46765578 ...  0.6535846  -0.26443586\n",
      "    0.9512202 ]\n",
      "  [-0.9917925   0.93422335 -0.15956964 ... -1.0005369  -1.1052504\n",
      "    1.0878924 ]\n",
      "  [-0.15341094 -0.49633235 -1.0531574  ... -1.0422709   0.14696114\n",
      "   -1.1754737 ]]\n",
      "\n",
      " [[ 0.14252743 -1.912647    0.3523504  ...  0.5547466   2.2784562\n",
      "   -0.47501934]\n",
      "  [ 0.52251136  0.02792656  0.4297811  ...  0.46887028 -0.618\n",
      "   -0.9332909 ]\n",
      "  [ 0.3738415  -1.0394145   0.36818567 ...  1.0868542   0.92440516\n",
      "   -0.7933633 ]]\n",
      "\n",
      " [[ 0.7373188  -0.79137135 -0.09440988 ... -0.2927476  -1.1766865\n",
      "    0.19765761]\n",
      "  [ 0.6182053  -1.3979286   1.0781604  ...  0.05028497  0.95728123\n",
      "   -0.35811925]\n",
      "  [-1.3685566   0.9251758   1.1725744  ...  0.26972452 -0.3785819\n",
      "   -0.56823355]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Visualizar el rendimiento durante el entrenamiento\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
